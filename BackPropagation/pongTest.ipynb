{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import ResizeObservation\n",
    "from gym.wrappers import FrameStack\n",
    "from gym.wrappers import TransformObservation\n",
    "import ale_py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "from collections import deque\n",
    "from qnetwork import QNetwork\n",
    "from agent import Agent\n",
    "from replay_buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Pong-v5', render_mode='rgb_array')\n",
    "done = True\n",
    "print('Press interrupt to stop execution')\n",
    "rewards = 0.0\n",
    "try:\n",
    "    for step in range(5000):\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state, reward, interupted, terminated, info = env.step(env.action_space.sample())\n",
    "            rewards += reward\n",
    "            done = interupted or terminated\n",
    "except KeyboardInterrupt:\n",
    "    print('Execution Interrupted.')\n",
    "finally:\n",
    "    env.close()\n",
    "print('Total Reward:', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structure the environment for processing\n",
    "env = GrayScaleObservation(env) # Converts the state to grayscale\n",
    "env = TransformObservation(env, lambda x: x / 255.0)  # scale pixel values to range [0, 1]\n",
    "env = ResizeObservation(env, 84) # Resizes the state into a 84x84.\n",
    "env = FrameStack(env, 4) # Stacks 4 frames \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation structure\n",
    "obs, _ = env.reset()\n",
    "obs = torch.Tensor(obs).squeeze()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot the observation\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(obs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(state, num):\n",
    "    state = np.array(state)\n",
    "    state = np.rollaxis(state, num, 0)  \n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "state_size = (4, 84, 84) # updated state size # ALE/Pong-v5 state size\n",
    "action_size = env.action_space.n\n",
    "BUFFER_SIZE = int(1e5) # replay buffer size\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99 # discount factor\n",
    "TAU = 1e-3 # for soft update of target parameters\n",
    "LR = 5e-4 # learning rate\n",
    "UPDATE_EVERY = 4 # how often to update the network\n",
    "max_t = 1000 # maximum number of timesteps per episode\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "# training loop\n",
    "scores = [] \n",
    "score_threshold = 21\n",
    "scores_window = deque(maxlen=100)\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.01\n",
    "number_of_episodes = 2000\n",
    "\n",
    "# Train the agent\n",
    "for i_episode in range(1, number_of_episodes+1):\n",
    "    env_info = env.reset()\n",
    "    state = env_info[0]\n",
    "    state = roll(state, 2)  \n",
    "    total_reward = 0\n",
    "    \n",
    "    # Decrease epsilon\n",
    "    epsilon = max(epsilon_min, epsilon_decay * epsilon)\n",
    "    \n",
    "    for t in range(max_t):\n",
    "        \n",
    "        action = agent.act(state, epsilon)\n",
    "        # env_info, reward, done, info, extra = env.step(action)\n",
    "        env_info, reward, interupted, terminated, info = env.step(action)\n",
    "\n",
    "        next_state = env_info\n",
    "        \n",
    "        print(\"Shape of the next_state before roll \", next_state.shape)\n",
    "        next_state = roll(next_state, 2)\n",
    "        print(\"Shape of the next_state after roll \", next_state.shape)\n",
    "        print(\"Shape of the next step in step agent \", state.shape)\n",
    "        agent.step(state, action, reward, next_state, interupted)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        if interupted:\n",
    "            break\n",
    "    \n",
    "    scores_window.append(total_reward)\n",
    "    scores.append(total_reward)\n",
    "    \n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    \n",
    "    if i_episode % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "    if np.mean(scores_window) >= score_threshold:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "        torch.save(agent.qnetwork_local.state_dict(), 'model.backpropagation.variant.data')\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
