{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from gym.wrappers import GrayScaleObservation\n",
    "from gym.wrappers import ResizeObservation\n",
    "from gym.wrappers import FrameStack\n",
    "from gym.wrappers import TransformObservation\n",
    "import ale_py\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.functional import F\n",
    "import torch.optim as optim \n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import deque\n",
    "import sys\n",
    "import importlib\n",
    "\n",
    "from collections import deque\n",
    "from qnetwork_genertic import QNetwork\n",
    "from genertic import GeneticAlgorithm\n",
    "from exp_replay import ExperienceReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Pong-v5', render_mode='rgb_array')\n",
    "done = True\n",
    "print('Press interrupt to stop execution')\n",
    "rewards = 0.0\n",
    "try:\n",
    "    for step in range(5000):\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state, reward, interupted, terminated, info = env.step(env.action_space.sample())\n",
    "            rewards += reward\n",
    "            done = interupted or terminated\n",
    "except KeyboardInterrupt:\n",
    "    print('Execution Interrupted.')\n",
    "finally:\n",
    "    env.close()\n",
    "print('Total Reward:', rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Structure the environment for processing\n",
    "env = GrayScaleObservation(env) # Converts the state to grayscale\n",
    "env = TransformObservation(env, lambda x: x / 255.0)  # scale pixel values to range [0, 1]\n",
    "env = ResizeObservation(env, 84) # Resizes the state into a 84x84.\n",
    "env = FrameStack(env, 4) # Stacks 4 frames \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation structure\n",
    "obs, _ = env.reset()\n",
    "obs = torch.Tensor(obs).squeeze()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot the observation\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for idx, ax in enumerate(axs.flatten()):\n",
    "    ax.imshow(obs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, q_network):\n",
    "    done = False\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.Tensor(obs).squeeze()\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            q_values = q_network(obs.unsqueeze(0))\n",
    "            action = torch.argmax(q_values).item()\n",
    "\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        obs = torch.Tensor(obs).squeeze()\n",
    "        total_reward += reward\n",
    "\n",
    "    return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_observation(obs):\n",
    "    # Structure the environment for processing\n",
    "    obs = GrayScaleObservation(obs) # Converts the state to grayscale\n",
    "    obs = TransformObservation(obs, lambda x: x / 255.0)  # scale pixel values to range [0, 1]\n",
    "    obs = ResizeObservation(obs, 84) # Resizes the state into a 84x84.\n",
    "    obs = FrameStack(obs, 4) # Stacks 4 frames\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "population_size = 20\n",
    "mutation_rate = 0.01\n",
    "num_generations = 10\n",
    "num_params = 768\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "num_eval_episodes = 3\n",
    "epsilon = 0.1\n",
    "replay_buffer_size = 10000\n",
    "\n",
    "# Experience Replay, and Genetic Algorithm\n",
    "experience_replay = ExperienceReplay(buffer_size=10000)\n",
    "genetic_algorithm = GeneticAlgorithm(population_size=100, mutation_rate=0.1)\n",
    "\n",
    "# Initialize genetic algorithm\n",
    "gen_alg = GeneticAlgorithm(population_size, mutation_rate)\n",
    "gen_alg.initialize_population(num_params)\n",
    "\n",
    "# Initialize Q-network and target network\n",
    "\n",
    "q_network = QNetwork(num_channels=4, num_out=6, hidden_size=128)\n",
    "target_network = QNetwork(num_channels=4, num_out=6, hidden_size=128)\n",
    "target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.001)\n",
    "loss_function = nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "# Train Q-network using genetic algorithm and experience replay\n",
    "for generation in range(num_generations):\n",
    "    print(f'Generation {generation + 1}')\n",
    "    \n",
    "    # Evaluate fitness of current population\n",
    "    fitness_scores = gen_alg.compute_fitness_scores(q_network, env, num_eval_episodes, epsilon)\n",
    "    \n",
    "    # Select parents\n",
    "    parents = gen_alg.select_parents(fitness_scores)\n",
    "    \n",
    "    # Generate children using crossover\n",
    "    children = gen_alg.crossover(parents)\n",
    "    \n",
    "    # Mutate children\n",
    "    mutated_children = gen_alg.mutate(children)\n",
    "    \n",
    "    # Merge parents and mutated children to form new population\n",
    "    new_population = np.concatenate([parents, mutated_children], axis=0)\n",
    "    \n",
    "    # Initialize episode\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Train Q-network using experience replay\n",
    "    while not done:\n",
    "        # Epsilon-greedy action\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state).unsqueeze(0).float()\n",
    "            with torch.no_grad():\n",
    "                q_values = q_network(state_tensor)\n",
    "                action = q_values.argmax().item()\n",
    "        \n",
    "        # Take action and add experience to replay buffer\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        experience_replay.add_experience(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        \n",
    "        # Sample batch from replay buffer and update Q-network\n",
    "        if experience_replay.size() >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = experience_replay.sample(batch_size)\n",
    "            \n",
    "            q_values = q_network(states)\n",
    "            next_q_values = target_network(next_states)\n",
    "            next_q_max_values, _ = next_q_values.max(dim=1, keepdim=True)\n",
    "            targets = rewards + gamma * next_q_max_values * (1 - dones)\n",
    "            \n",
    "            q_values = q_values.gather(dim=1, index=actions.unsqueeze(1))\n",
    "            loss = loss_function(q_values, targets)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update target network\n",
    "            for target_param, param in zip(target_network.parameters(), q_network.parameters()):\n",
    "                target_param.data.copy_(target_param.data * (1.0 - 0.001) + param.data * 0.001)\n",
    "    \n",
    "    print(f'Episode reward: {episode_reward}')\n",
    "    \n",
    "    # Update Q-network weights\n",
    "    for target_param, param in zip(q_network.parameters(), new_population):\n",
    "        target_param.data.copy_(param)\n",
    "\n",
    "    # Set target network weights equal to Q-network weights\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Reset epsilon to initial value\n",
    "    epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(q_network, env, num_episodes=10):\n",
    "    total_reward = 0\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = q_network.choose_action(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "    avg_reward = total_reward / num_episodes\n",
    "    return avg_reward\n",
    "\n",
    "# Evaluate the performance of the trained QNetwork\n",
    "avg_reward = evaluate_performance(q_network, env, num_episodes=100)\n",
    "print(f\"Average Reward: {avg_reward}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
