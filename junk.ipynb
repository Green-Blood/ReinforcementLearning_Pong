{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import deque\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import gym\n",
    "# import cv2\n",
    "# import matplotlib.pyplot as plt\n",
    "# from agent import Agent\n",
    "# from replay_buffer import ReplayBuffer\n",
    "# from qnetwork import QNetwork\n",
    "\n",
    "# # Define constants \n",
    "# env = gym.make('ALE/Pong-v5', render_mode='rgb_array')\n",
    "# MEAN_REWARD_BOUND = 19.5\n",
    "# # Number of games to play for training\n",
    "# NUM_EPISODES = 10\n",
    "# # Number of frames to skip\n",
    "# FRAME_SKIP = 4\n",
    "# # Size of replay buffer\n",
    "# BUFFER_SIZE = 10000\n",
    "# # Number of steps to train on each episode\n",
    "# BATCH_SIZE = 32\n",
    "# # Discount factor\n",
    "# GAMMA = 0.99\n",
    "# # Soft update parameter for target network\n",
    "# TAU = 0.01\n",
    "# # Learning rate for optimizer\n",
    "# LEARNING_RATE = 0.0001\n",
    "# # how often to update the network\n",
    "# UPDATE_EVERY = 4        \n",
    "# state_size=4\n",
    "# action_size=2\n",
    "# seed=0\n",
    "\n",
    "# agent = Agent(state_size, action_size, seed, BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LEARNING_RATE, UPDATE_EVERY)\n",
    "# qnetwork_local = QNetwork(state_size, action_size, seed)\n",
    "# qnetwork_target = QNetwork(state_size, action_size, seed)\n",
    "# replay_buffer = ReplayBuffer(action_size=2, buffer_size=BUFFER_SIZE, batch_size=BATCH_SIZE, seed=0)\n",
    "\n",
    "# # Keep track of scores\n",
    "# scores = []\n",
    "# scores_window = deque(maxlen=100)  # last 100 scores\n",
    "# eps = 1.0                          # initialize epsilon\n",
    "\n",
    "# # Train the agent\n",
    "# for i_episode in range(1, 2001):\n",
    "#     env_info = env.reset()\n",
    "#     state = env.render(mode='rgb_array')\n",
    "#     total_reward = 0\n",
    "    \n",
    "#     # Decrease epsilon\n",
    "#     eps = max(0.01, 0.995*eps)\n",
    "    \n",
    "#     # Play a game\n",
    "#     while True:\n",
    "#         # Choose an action\n",
    "#         action = agent.act(state, eps)\n",
    "        \n",
    "#         # Take the action\n",
    "#         env_info = env.step(action)\n",
    "#         next_state = env.render(mode='rgb_array')\n",
    "#         reward = env_info['reward']\n",
    "#         done = env_info['game_over']\n",
    "        \n",
    "#         # Add the experience to the replay buffer\n",
    "#         replay_buffer.add(state, action, reward, next_state, done)\n",
    "        \n",
    "#         # If the replay buffer has enough experiences, train the agent\n",
    "#         if len(replay_buffer) > BATCH_SIZE:\n",
    "#             experiences = replay_buffer.sample()\n",
    "#             agent.learn(experiences, GAMMA, qnetwork_local, qnetwork_target, TAU, LEARNING_RATE, UPDATE_EVERY)\n",
    "        \n",
    "#         state = next_state\n",
    "#         total_reward += reward\n",
    "        \n",
    "#         if done:\n",
    "#             break\n",
    "            \n",
    "#     scores.append(total_reward)\n",
    "#     scores_window.append(total_reward)\n",
    "    \n",
    "#     # Print the average score over the last 100 episodes\n",
    "#     print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "    \n",
    "#     # If the average score is greater than 21, save the model and break the loop\n",
    "#     if np.mean(scores_window) >= 21.0:\n",
    "#         print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "#         torch.save(qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "# # # Define preprocessing function\n",
    "# # def preprocess_frame(frame):\n",
    "# #     \"\"\"Preprocesses the frame for input to the QNetwork.\"\"\"\n",
    "# #     # Convert the frame to grayscale\n",
    "# #     frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "# #     # Resize the frame to 84x84\n",
    "# #     frame = cv2.resize(frame, (84, 84))\n",
    "# #     # Scale the pixel values to between 0 and 1\n",
    "# #     frame = frame.astype(np.float32) / 255.0\n",
    "# #     return frame\n",
    "\n",
    "# # # Define function for getting the epsilon-greedy action\n",
    "# # def get_action(q_network, obs, epsilon, device):\n",
    "# #     \"\"\"Returns the action to take given the observation and the epsilon value.\"\"\"\n",
    "# #     if np.random.random() < epsilon:\n",
    "# #         # Choose a random action\n",
    "# #         action = env.action_space.sample()\n",
    "# #     else:\n",
    "# #         # Choose the action with the highest Q-value\n",
    "# #         obs_v = torch.tensor([obs]).to(device)\n",
    "# #         q_values_v = q_network(obs_v)\n",
    "# #         _, act_v = torch.max(q_values_v, dim=1)\n",
    "# #         action = int(act_v.item())\n",
    "# #     return action\n",
    "\n",
    "# # # Define the main training loop\n",
    "# # def train(q_network, target_network, replay_buffer, device):\n",
    "# #     \"\"\"Runs the main training loop for the QNetwork.\"\"\"\n",
    "# #     optimizer = optim.Adam(q_network.parameters(), lr=LEARNING_RATE)\n",
    "# #     obs = env.reset()\n",
    "# #     total_rewards = []\n",
    "# #     for episode in range(NUM_EPISODES):\n",
    "# #         # Reset the environment and get the initial observation\n",
    "# #         obs = env.reset()\n",
    "# #         # Convert the initial observation to grayscale and preprocess it\n",
    "# #         obs = preprocess_frame(obs)\n",
    "# #         # Stack the preprocessed observation 4 times to get the initial state\n",
    "# #         state = np.stack([obs] * FRAME_SKIP, axis=0)\n",
    "# #         total_reward = 0.0\n",
    "# #         done = False\n",
    "# #         while not done:\n",
    "# #             # Get an action to take\n",
    "# #             epsilon = max(1.0 - episode * 0.01, 0.1)\n",
    "# #             action = get_action(q_network, state, epsilon, device)\n",
    "# #             # Take the action and get the new observation, reward, and done flag\n",
    "# #             next_obs, reward, done, _ = env.step(action)\n",
    "# #             # Convert the new observation to grayscale and preprocess it\n",
    "# #             next_obs = preprocess_frame(next_obs)\n",
    "# #             # Remove the oldest frame and add the new frame to the state\n",
    "# #             next_state = np.concatenate([state[1:], np.expand_dims(next_obs, axis=0)], axis=0)\n",
    "# #             # Add the experience to the replay buffer\n",
    "# #             replay_buffer.add(state, action, reward, next_state, done)\n",
    "# #             state = next_state\n",
    "# #             total_reward += reward\n",
    "# #             # If the replay buffer has enough experiences, train the agent\n",
    "# #             if len(replay_buffer) > BATCH_SIZE:\n",
    "# #                 experiences = replay_buffer.sample(BATCH_SIZE)\n",
    "# #                 agent.learn(experiences, GAMMA)\n",
    "\n",
    "# #             # If it's time to update the target network, do so\n",
    "# #             if t % target_update_freq == 0:\n",
    "# #                 agent.soft_update(agent.qnetwork_local, agent.qnetwork_target, tau)\n",
    "            \n",
    "# #             if done:\n",
    "# #                 break\n",
    "            \n",
    "# #         # Append the total reward for this episode to the list of rewards\n",
    "# #         rewards_list.append(total_reward)\n",
    "        \n",
    "# #         # Print some statistics every few episodes\n",
    "# #         if i_episode % print_every == 0:\n",
    "# #             print(\"Episode: {}, average reward over the last {} episodes: {:.2f}\".format(i_episode, print_every, np.mean(rewards_list[-print_every:])))\n",
    "            \n",
    "# #         # Check if the environment is solved\n",
    "# #         if np.mean(rewards_list[-100:]) >= target_reward:\n",
    "# #             print(\"Environment solved in {} episodes with an average reward of {:.2f} over the last {} episodes\".format(i_episode-100, np.mean(rewards_list[-100:]), 100))\n",
    "# #             break\n",
    "        \n",
    "# #     # Return the list of rewards\n",
    "# #     return rewards_list\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7cb1b9ae4d417fedf7f40a8eec98f7cfbd359e096bd857395a915f4609834ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
